{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. Predicting Insurance Cost.\n",
    "\n",
    "The goal for this problem is to get you to do the whole process of splitting data, train a whole bunch of model, do validation and report the **performance**.\n",
    "\n",
    "Using the data from kaggle. https://www.kaggle.com/mirichoi0218/insurance/home\n",
    "\n",
    "**Part 1)** \n",
    "Write a function \n",
    "```python\n",
    "def predict_insurance(features):\n",
    "    return predicted_insurance_cost\n",
    "```\n",
    "\n",
    "Specification/Hint:\n",
    " - Using linear function of features is a bad idea since features such as BMI has a healthy value in the middle. Do something more fancy.\n",
    " - You must transform categorical feature in to ordinal feature. (unless there are 2 choices)\n",
    " - You must train and validate your model.\n",
    " - Explain what you are doing and plot something to show that what you are doing is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2)**\n",
    "\n",
    "Predicting the number alone is not very useful. We can do even better by reporting the number like\n",
    "$$\n",
    "    guess \\pm uncertainty\n",
    "$$\n",
    "where the uncertainty should mean something along the line of \"confidence interval\". There are many ways to do this.\n",
    "\n",
    "Compute the uncertainty and briefly justify your method of calculating the uncertainty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. Lasso: Gems' idea wan't so crazy after all.\n",
    "\n",
    "In the class we learn regularization with $\\sum w^2$. The fancy name for $\\sum w^2$ is called $L_2$ [norm](https://en.wikipedia.org/wiki/Norm_(mathematics)). The resulting regression is called Ridge Regression or $L_2$ regularization.\n",
    "\n",
    "In this problem we will learn about regularization using $L_1$ norm$ \\sum |w|$. The resulting regression is called Lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1)\n",
    "Let us consider a problem of least square fitting a line($guess(x) = mx + c$) to data points $x^{(i)}, y^{(i)}$ for $i=1\\ldots n$ with $L_2$ regularization.\n",
    "\n",
    "### Problem 1.1)\n",
    "Write down the cost function in terms of the given variables. $x^{(i)}, y^{(i)}, m, c, \\lambda$\n",
    "$$\n",
    "cost(m, c) = \\sum_{i=1}^{n}\\ldots + \\lambda \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2)\n",
    "We are interested in solution for $m$ which minimize the cost function. Show that $m$ must satisfy the following equation.\n",
    "\n",
    "$$\n",
    "    m + \\frac{\\lambda}{B} m = C\n",
    "$$\n",
    "\n",
    "Find B, C in terms of $x^{(i)}, y^{(i)}, c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3)\n",
    "\n",
    "It's clear from the above equation that the solution when $\\lambda = 0$ is $m = C$. We will called this $m_{old}$. \n",
    "\n",
    "The goal is to compare the solution when there is $L_2$ regularization($m_{new}$) with the solution when there is no regularization($m_{old}$).\n",
    "\n",
    "**Your task** Show that (find the $\\ldots$ part in terms of $\\lambda, B, C$)\n",
    "\n",
    "$$\n",
    "    m_{new} = \\frac{m_{old}}{\\ldots}\n",
    "$$\n",
    "\n",
    "This means that what $L_2$ regularization does is just rotating/scaling equally the solution so that it becomes nearer to zero. This is the desired effect since we notice that large $w$ typically leads to overfitting.\n",
    "\n",
    "<img src=\"l2reg.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4\n",
    "\n",
    "Make the right answer **bold**.\n",
    "\n",
    "1) If $\\lambda$ is large, the answer will get rotated (more/less/same).\n",
    "\n",
    "2) The larger $m_{old}$, the new answer from regularization will get *shifted*($abs(m_{new}-m_{old})$) by (more/less/the same) amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "$L_1$ regularization where we use $\\sum |w|$ instead of $\\sum{w^2}$ for regularization term has a similar but stronger effect which is desirable sometimes. This problem will guide you through understanding the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.1\n",
    "\n",
    "Let us consider the same problem of least square fitting a line($guess(x) = mx + c$) to data points $x^{(i)}, y^{(i)}$ for $i=1\\ldots n$ but now with $L_1$ regularization.\n",
    "\n",
    "### Problem 2.1)\n",
    "Write down the cost function in terms of the given variables. $x^{(i)}, y^{(i)}, m, c, \\lambda$\n",
    "$$\n",
    "cost(m, c) = \\sum_{i=1}^{n}\\ldots + \\lambda \\ldots\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2)\n",
    "We are interested in solution for $m$ which minimize the cost function. Show that $m$ must satisfy the following equation.\n",
    "\n",
    "$$\n",
    "    m + \\frac{\\lambda}{B} \\text{sgn}(m) = C\n",
    "$$\n",
    "where $\\text{sgn(m)}$ is a sign function (1 if $m$ is positive, -1 if $m$ is negative, 0 if $m$ is zero)\n",
    "\n",
    "Find B, C in terms of $x^{(i)}, y^{(i)}, c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3)\n",
    "\n",
    "It's clear from the above equation that the solution when $\\lambda = 0$ is $m = C$. We will called this $m_{old}$. \n",
    "\n",
    "The goal is to compare the solution when there is $L_1$ regularization($m_{reg}$) with the solution when there is no regularization($m_{old}$).\n",
    "\n",
    "Once you sketch the plot of $m_{old}$ on $x$ axis against $m_{reg}$ on $y$ axis. You should get a graph that looks like the following.\n",
    "\n",
    "(In the plot $x_{old}$ is $m_{old}$ and $x_{new}$ is $m_{new}$ )\n",
    "\n",
    "<img src=\"l1reg.png\" width=\"400\"/>\n",
    "\n",
    "This means that $L_1$ regularization suppress small value of $m_{old}$ to zero and *shift* the rest down by an equal amount\n",
    "\n",
    "**Your task**  Find $x^+$ and $x^-$ and $\\delta$ in terms of $\\lambda, B, C$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4\n",
    "Make the right answer bold.\n",
    "From the answer you got in 2.3.\n",
    "\n",
    "1) If we make $\\lambda$ larger, the suppressed range $|x^+-x^-|$ will be (wider,narrower,unaffected).\n",
    "\n",
    "2) If we make $\\lambda$ larger, ther shifted amount $\\delta$ will be (wider,narrower,unaffected).\n",
    "\n",
    "3) Outside the suppressed range, for larger $x_{old}$ the shifted amount will be (wider,narrower,unaffected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue.\n",
    "\n",
    "Let us consider a useless feature. The weight for this feature is typically small.\n",
    "\n",
    "$L_2$ regularization will make the weight for this feature smaller. But, $L_1$ supression will bring this down to exactly zero thus completely eliminating useless feature. But the bad things is $\\lambda$ for $L_1$ affect both the suppressed range and the shifted amount. \n",
    "\n",
    "Good news is we can control this by the linear combination of the two\n",
    "$\\lambda_1 \\sum |w| + \\lambda_2 \\sum w^2$. \n",
    "\n",
    "Or some sort of piece wise function ex: [Huber loss](https://en.wikipedia.org/wiki/Huber_loss).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11.9886px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
